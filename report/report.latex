\documentclass{article}
\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{graphicx}
\graphicspath{{../graphs/}}

\begin{document}
\title{Mathematics for Machine Learning --- Coursework 4}
\date{\today}
\author{Balint Rikker}

\maketitle

\paragraph{Part I} \mbox{} \\

See \cref{fig:pca} for PCA, \cref{fig:wpca} for whitened PCA, and \cref{fig:lda} for LDA.
Overall LDA seem to perform the best for Small Sample Size problems, but it requires labels being available.
For unsupervised problems, whitened PCA seem to perform better than PCA.

\begin{figure}[h]
\caption{PCA recognition error vs number of components kept}
\centering
\label{fig:pca}
\includegraphics[width=0.8\textwidth]{pca}
\end{figure}

\begin{figure}[h]
\caption{Whitened PCA recognition error vs number of components kept}
\centering
\label{fig:wpca}
\includegraphics[width=0.8\textwidth]{wpca}
\end{figure}

\begin{figure}[h]
\caption{LDA recognition error vs number of components kept}
\centering
\label{fig:lda}
\includegraphics[width=0.8\textwidth]{lda}
\end{figure}

\paragraph{Part II} \mbox{} \\

The Lagrangian of the optimization problem can be stated as follows (where $a_i \geq 0$ and $r_i \geq 0$ are Lagrangian multipliers):

\begin{equation} \label{eq:L}
    L(\textbf{w}, b, \xi_i, a_i, r_i) = \frac{1}{2} \textbf{w}^T \textbf{S}_t \textbf{w} + C \sum^n_{i=1}\xi_i - \sum^n_{i=1} a_i (y_i (\textbf{w}^T \textbf{x}_i + b) - 1 + \xi_i) - \sum^n_{i=1}r_i\xi_i
\end{equation}

The dual problem to optimize is then:

$$ \max_{a_i > 0} \min_{\textbf{w}, b, \xi_i} L(\textbf{w}, b, \xi_i, a_i, r_i) $$

The optimal $\textbf{w}$, $b$ and $\xi_i$ will satisfy the condition that the partial derivatives with regards to these parameters will be 0.

$$ \frac{\partial L}{\partial \textbf{w}} = \frac{1}{2} (\textbf{S}_t + \textbf{S}_t^T) \textbf{w} - \sum_{i=1}^{n} a_i y_i \textbf{x}_i = \textbf{S}_t \textbf{w} - \sum_{i=1}^{n} a_i y_i \textbf{x}_i = 0 \Leftrightarrow $$
$$ \textbf{S}_t \textbf{w} = \sum_{i=1}^{n} a_i y_i \textbf{x}_i \Leftrightarrow $$
$$ \textbf{w} = \textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i $$

\bigskip

$$ \frac{\partial L}{\partial b} = \sum_{i=1}{n}a_i y_i = 0 \Leftrightarrow $$
$$ \textbf{a}^T \textbf{y} = 0 $$

\bigskip

$$ \frac{\partial L}{\partial \xi_i} = C - a_i - r_i = 0 \Leftrightarrow $$
$$ r_i + a_i = C $$


We can substitute these values back to \labelcref{eq:L}:

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{S}_t (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i) + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i^T) \textbf{x}_i + b) - 1 + \xi_i) - \sum_{i=1}^{n}r_i \xi_i $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i+ b)) + \sum_{i=1}^{n} a_i - \sum_{i=1}^{n}a_i \xi_i - \sum_{i=1}^{n}r_i \xi_i $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i+ b)) + \sum_{i=1}^{n} a_i - \sum_{i=1}^{n}a_i \xi_i - \sum_{i=1}^{n}r_i \xi_i $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i  - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i+ b)) + \sum_{i=1}^{n} a_i + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n}\xi_i (a_i + r_i) $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i  - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i+ b)) + \sum_{i=1}^{n} a_i + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n}\xi_i C $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i  - \sum_{i=1}^{n} a_i (y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i+ b)) + \sum_{i=1}^{n} a_i $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i  - \sum_{i=1}^{n} a_i y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i) + \sum_{i=1}^{n} a_i y_i b + \sum_{i=1}^{n} a_i $$

$$ L(\textbf{a}) = \frac{1}{2} (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \sum_{i=1}^{n} a_i y_i \textbf{x}_i  - \sum_{i=1}^{n} a_i y_i ( (\textbf{S}_t^{-1} \sum_{i=1}^{n} a_i y_i \textbf{x}_i)^T \textbf{x}_i) + \sum_{i=1}^{n} a_i $$

$$ L(\textbf{a}) = -\frac{1}{2} \sum_{i=1}^{n}\sum_{j=1}^{n}a_iy_i \textbf{x}_i^T \textbf{S}_t^{-1} a_j y_j \textbf{x}_j + \sum_{i=1}^{n} a_i $$

\end{document}
